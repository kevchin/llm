{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc23fffe-51b3-4424-9e22-c61438f0d53c",
   "metadata": {},
   "source": [
    "## From:\n",
    "  * https://youtu.be/dD_xNmePdd0?t=1163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1de9d5f-6844-452b-9ba4-6b2e066dbcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781ec6bb-0ef4-40a3-b94f-5083d70f050a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0044b6f6-d06b-4262-8e9e-7670de0406b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('huggingface.ini')\n",
    "token = config['DEFAULT']['API_Key']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6090a-6cac-4293-942c-d2d37b21500e",
   "metadata": {},
   "source": [
    "## See:\n",
    "  * https://huggingface.co/openai-gpt\n",
    "  * Size = 479MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec2662f-ec3d-4917-82ac-284ee84d4256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, for'e's a very big one. they call me'e'n e'n''\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, \" he said and leaned his left shoulder against the corner of the door. \\n i couldn\\'t take my'},\n",
       " {'generated_text': \"Hello, I'm a language model,'came their reply. \\n'it's a man thing! if you have three hundred thousand words,\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, \" he said, the sound of his voice sending a shiver down her spine. \\n \" a language model'},\n",
       " {'generated_text': \"Hello, I'm a language model,'he said.'you call me a'man out'in... well, er, in the\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='openai-gpt')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3da0d9-b3c3-4867-a258-2da4b1ba7c43",
   "metadata": {},
   "source": [
    "## Use GPT2\n",
    "  * https://huggingface.co/transformers/v3.0.2/model_doc/gpt2.html?highlight=gpt2tokenizer#transformers.GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05e1258-0671-452c-83f0-74bd208532d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdaaacc2-03ed-432b-8018-ac9439cc1492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a48d89-a58c-420f-9cd1-9f1bdde351b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c67f2d-3a86-4352-a845-ce4224e7d750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791b6d3e-f8bf-432c-8b06-7068136aa5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf881a0-2cd9-4de2-bc9a-8bf1c2e0c747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a362fb-72eb-44c9-ada0-a2098aed516f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbeacaf-6287-480d-a903-fc2817023f56",
   "metadata": {},
   "source": [
    "## Try Using larger model:\n",
    "  * https://huggingface.co/transformers/v3.0.2/pretrained_models.html\n",
    "  * gpt2-medium = 1.5GB\n",
    "  * 24 layer, 1024 hidden, 16 heads, 345M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6e057d-f1dc-4a2e-9b94-4a0d6d5e3ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1349c1b8d647a2ae2ba4a692ee5f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6d15df717c476f8d6bcd88c58862ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01c1ba78c5e44a4851e1c94fb010a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d29aef6953f4be9b770f915e1952fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86083822766425ca4d4318a32265de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a220543ef84be78a3881fb235c2594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a language model for people. There are people who can learn an expressive\"},\n",
       " {'generated_text': \"Hello, I'm a language model, this is my first blogpost.\\n\\nLanguage models are very fundamental to software development and are essential to maintaining\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do anything in my power to help improve the language as it's being designed. I have over\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, not a developer.\"\\n\\nI\\'m not.\\n\\nI\\'ve spent more than five years doing what I'},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using them in all sorts of scenarios for a while and I still have issues – it's pretty\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2-medium')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "029c06b7-256d-4f8e-8b7a-abfd533ab767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7fc527-ddb6-41ec-9760-5dd8a6e5e2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I can do it with my cat. I'm not sure if I can do it with my cat.\n",
      "\n",
      "I'm not sure if I can do it with my cat.\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fce04e-ce9d-4bd4-8b05-65555ae09912",
   "metadata": {},
   "source": [
    "## Done - ZZZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3753ae-27cc-4a36-9580-59f3601b2eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
