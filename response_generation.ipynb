{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82082223-d410-464f-bf20-0f56ddcb73f4",
   "metadata": {},
   "source": [
    "## LLM - Text Generation From: \n",
    "  * https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/opt\n",
    "  * https://huggingface.co/KoboldAI/OPT-350M-Erebus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09053618-7858-439e-8588-8eb6c30a712e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8114753-2eb6-444c-bfbd-d03f9bc9298e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = 'KoboldAI/OPT-350M-Erebus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0087b75-6640-4738-a797-d2472f8de3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = OPTForCausalLM.from_pretrained(model1)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0af15b5-2d7d-4a6e-ae55-2a25cc642679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you awake? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3426952c-42f6-4709-ace1-99a62ede936f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you awake? Can you talk to me? I need to talk to you.\"\n",
      "\"I'm awake, I'm sorry,\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "seq = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print (seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c9ed2a-d98a-4545-8eec-92cc2d5c0bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"When do you think that software be completed and tested?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6d49cd-c67b-475e-8f2e-e59f2865709f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When do you think that software be completed and tested?\n",
      "I'm not sure. I'm not sure if I can get it done in time\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "seq = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print (seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb68d9d5-595b-4214-b790-4654c1df672b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Hi did you finish your homework already? I don't want you watching TV until you finish your homework\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9368d5-8f31-448f-a513-911737f4b7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi did you finish your homework already? I don't want you watching TV until you finish your homework.\"\n",
      "\"Yes, I finished it.\"\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "seq = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print (seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1816f-050a-4b90-858c-0a864e5a46c5",
   "metadata": {},
   "source": [
    "# Done - ZZZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a145c62-97f8-4092-a6aa-6cdd846b1c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
